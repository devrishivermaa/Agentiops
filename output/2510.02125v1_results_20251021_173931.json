{
  "SM-41B410": {
    "status": "ok",
    "output": {
      "sm_id": "SM-41B410",
      "role": "Summarize Abstract and Introduction to provide context and overview.",
      "assigned_sections": [
        "Abstract",
        "Introduction"
      ],
      "page_range": [
        1,
        8
      ],
      "results": [
        {
          "page": 1,
          "section": "Abstract",
          "char_count": 3336,
          "text_preview": "Preprint. Under Review\nDOAI MODELSPERFORMHUMAN-LIKEABSTRACT\nREASONINGACROSSMODALITIES?\nClaas Beger\nSanta Fe Institute\nclaasbeger@santafe.edu\nRyan Yi\nSanta Fe Institute\nryi@santafe.edu\nShuhao Fu\nSanta Fe Institute\nfushuhao@santafe.edu\nArseny Moskvichev\nAdvanced Micro Devices, Inc.\narseny.moskvichev@g...",
          "summary": "This paper investigates the abstract reasoning capabilities of AI models, particularly OpenAI's o3-preview, using the ConceptARC benchmark. While some text-based models achieve human-level accuracy, their reasoning often relies on surface-level shortcuts rather than intended abstractions. Visual models show lower accuracy but a greater proportion of abstract rules, suggesting current evaluations may overestimate text-based and underestimate visual abstract reasoning.",
          "entities": [
            "OpenAI",
            "o3-preview",
            "ARC-AGI benchmark",
            "ConceptARC benchmark",
            "Claas Beger",
            "Ryan Yi",
            "Shuhao Fu",
            "Arseny Moskvichev",
            "Sarah W. Tsai",
            "Sivasankaran Rajamanickam",
            "Melanie Mitchell",
            "Santa Fe Institute",
            "Advanced Micro Devices, Inc.",
            "Sandia National Laboratories",
            "Abstraction and Reasoning Corpus (ARC)",
            "Chollet (2019)"
          ],
          "keywords": [
            "abstract reasoning",
            "AI models",
            "ConceptARC benchmark",
            "multimodal",
            "textual modality",
            "visual modality",
            "rule-level analysis",
            "surface-level shortcuts",
            "abstraction abilities",
            "human accuracy"
          ],
          "key_points": [
            "OpenAI's o3-preview model exceeded human accuracy on ARC-AGI, but its abstract reasoning is questioned.",
            "The study uses the ConceptARC benchmark to evaluate AI models' abstraction abilities across different input modalities (textual vs. visual) and tool usage.",
            "Fine-grained evaluation of generated natural-language rules is performed alongside output accuracy.",
            "Text-based models often use surface-level shortcuts, overestimating their abstract reasoning capabilities.",
            "Visual models show lower accuracy but a higher proportion of intended abstract rules, suggesting underestimation by accuracy alone.",
            "The evaluation framework aims for a more faithful assessment of multimodal abstract reasoning and progress towards human-like intelligence."
          ],
          "technical_terms": [
            "o3-preview reasoning model",
            "ARC-AGI benchmark",
            "ConceptARC benchmark",
            "input modality",
            "external Python tools",
            "reasoning effort",
            "output accuracy",
            "natural-language rules",
            "surface-level patterns",
            "Abstraction and Reasoning Corpus (ARC)",
            "few-shot rule-induction",
            "analogical reasoning",
            "testgrid",
            "multimodal models"
          ],
          "status": "success"
        },
        {
          "page": 2,
          "section": "Introduction",
          "char_count": 4750,
          "text_preview": "Preprint. Under Review\nTo solve a task, an agent should infer a rule governing the demonstrations and apply that rule to the\ntest input to produce a correct output grid.\nChollet 2025 devised 1,000 such tasks, releasing 400 easier puzzles as a “training set,” 400 harder\npuzzles as an “evaluation set,...",
          "summary": "This section introduces the Abstract Reasoning Corpus (ARC) and the challenges of achieving human-like abstract reasoning in AI. It highlights the performance of OpenAI's o3 model on ARC tasks, noting its significant accuracy but questioning whether it uses generalizable abstractions or superficial shortcuts. The research aims to assess the reasoning strategies of various AI models on the ConceptARC benchmark, investigating the impact of modality, reasoning effort, and tool access.",
          "entities": [
            "Chollet",
            "ARC-AGI Prize",
            "OpenAI",
            "o3 model",
            "ConceptARC",
            "Moskvichev et al."
          ],
          "keywords": [
            "abstract reasoning",
            "ARC tasks",
            "generalizable abstractions",
            "shortcuts",
            "ConceptARC benchmark",
            "modality",
            "reasoning effort",
            "Python tool access"
          ],
          "key_points": [
            "AI agents need to infer and apply rules to solve tasks.",
            "The ARC dataset consists of 1,000 tasks divided into training, evaluation, and private test sets.",
            "The ARC-AGI Prize competition aimed to find programs exceeding 85% accuracy on private test sets.",
            "OpenAI's o3 model achieved high accuracy (76-88%) on a semi-private test set, considered a breakthrough.",
            "It's unclear if AI systems like o3 use genuine abstractions or superficial shortcuts.",
            "The research uses ConceptARC, a benchmark focused on basic spatial and semantic concepts, to investigate AI reasoning.",
            "The study examines the impact of modality (textual vs. visual), reasoning effort, and tool access on AI performance."
          ],
          "technical_terms": [
            "abstract reasoning",
            "output grid",
            "LLM",
            "data augmentation",
            "accuracy",
            "private test set",
            "semi-private test set",
            "low-effort setting",
            "high-effort setting",
            "computing cost",
            "generalize",
            "instantiations",
            "abstract concepts",
            "shortcuts",
            "unintended correlations",
            "commercial models",
            "open-weight models",
            "ConceptARC",
            "spatial concepts",
            "semantic concepts",
            "varying contexts",
            "generalization",
            "robust understanding",
            "text-based representations",
            "integer matrix",
            "visual modalities",
            "reasoning stage",
            "token budget",
            "Python code"
          ],
          "status": "success"
        },
        {
          "page": 3,
          "section": "Introduction",
          "char_count": 2914,
          "text_preview": "Preprint. Under Review\nFigure 1: Each row shows a task from the ConceptARC benchmark. Each task shown consists of\nthree demonstrations of a transformation and one test grid. In this study, the solver is tasked with\ngenerating a rule that describes the transformations and applying that rule to the te...",
          "summary": "This section details the methodology for evaluating AI models on the ConceptARC benchmark, a dataset designed for testing abstract reasoning. It describes the dataset's creation and the selection of both proprietary \"reasoning\" and non-reasoning multimodal AI models for evaluation, along with the experimental setup and evaluation criteria.",
          "entities": [
            "ConceptARC benchmark",
            "Moskvichev et al. 2023",
            "OpenAI",
            "o3",
            "o4-mini",
            "Google",
            "Gemini 2.5 Pro",
            "Anthropic",
            "Claude Sonnet 4",
            "GPT-4o",
            "Meta",
            "Llama 4 Scout",
            "Alibaba",
            "Qwen 2.5 VL 72B",
            "Chollet et al.’s 2024",
            "Prolific Academic platform"
          ],
          "keywords": [
            "ConceptARC benchmark",
            "abstract concepts",
            "multimodal models",
            "reasoning models",
            "non-reasoning models",
            "transformation rule",
            "output grid",
            "grid output accuracy",
            "pass@1 results",
            "textual modality",
            "visual modality"
          ],
          "key_points": [
            "ConceptARC consists of 480 tasks based on 16 basic spatial and semantic concepts, designed to be relatively easy for humans.",
            "Four proprietary multimodal \"reasoning\" models (o3, o4-mini, Gemini 2.5 Pro, Claude Sonnet 4) and three non-reasoning multimodal models (GPT-4o, Llama 4 Scout, Qwen 2.5 VL 72B) were evaluated.",
            "Models were tasked with generating a JSON object containing a transformation rule and the corresponding output grid.",
            "Evaluation criteria include grid output accuracy and the degree to which generated rules capture intended abstractions.",
            "pass@1 results are reported for both AI models and humans, with context windows reset for each task."
          ],
          "technical_terms": [
            "spatial concepts",
            "semantic concepts",
            "abstraction",
            "multimodal models",
            "reasoning models",
            "non-reasoning models",
            "temperature",
            "textual modality",
            "visual modality",
            "JSON object",
            "transformation rule",
            "output grid",
            "matrix of integers",
            "context window",
            "pass@1"
          ],
          "status": "success"
        },
        {
          "page": 4,
          "section": "Introduction",
          "char_count": 4904,
          "text_preview": "Preprint. Under Review\nWe evaluated o3 under its low- and medium-effort reasoning settings.5 We evaluated Gemini 2.5\nPro and Claude Sonnet 4 with a reasoning budget of 16,000 tokens, chosen to roughly approximate\nOpenAI’s medium-effort setting. Additionally, across all reasoning models and modalitie...",
          "summary": "This section details the evaluation methodology for AI models on the ConceptARC dataset, focusing on both output-grid accuracy and the generation of natural-language rules describing the transformations. The study aims to distinguish between models that grasp abstract concepts and those that exploit superficial patterns, using human judgment to assess the quality of these rules.",
          "entities": [
            "o3",
            "Gemini 2.5 Pro",
            "Claude Sonnet 4",
            "OpenAI",
            "ConceptARC",
            "Moskvichev et al. 2023"
          ],
          "keywords": [
            "output-grid accuracy",
            "natural-language rule",
            "abstract concepts",
            "superficial patterns",
            "human judgment",
            "reasoning settings",
            "tool-access conditions",
            "demonstrations",
            "ground-truth solution"
          ],
          "key_points": [
            "AI models (o3, Gemini 2.5 Pro, Claude Sonnet 4) were evaluated on ConceptARC with and without Python tools.",
            "Evaluation includes output-grid accuracy and natural-language rule generation to assess understanding of abstract concepts versus superficial patterns.",
            "Human judgment is used to categorize generated rules as 'incorrect', 'correct-unintended', or 'correct-intended'.",
            "Examples of human and model-generated rules are provided to illustrate the evaluation criteria."
          ],
          "technical_terms": [
            "reasoning settings",
            "reasoning budget",
            "modalities",
            "tool-access conditions",
            "output-grid accuracy",
            "ground-truth solution",
            "ConceptARC corpus",
            "abstract concepts",
            "superficial patterns",
            "shortcuts",
            "large neural-network models",
            "natural-language rule",
            "demonstrations",
            "intended abstractions",
            "textual inputs",
            "visual input",
            "minimal bounding box",
            "token budget"
          ],
          "status": "success"
        },
        {
          "page": 5,
          "section": "Introduction",
          "char_count": 3567,
          "text_preview": "Preprint. Under Review\nTable 1:Reasoning models:Output-grid accuracy (pass@1) for Concept-ARC across models and\nexperimental settings. Accuracy is shown in %. Each cell showstextual / visualaccuracy. For o3 and\no4-mini, we use the “low” and “medium” effort settings in the OpenAI API. For Claude and ...",
          "summary": "Page 5 presents results from reasoning models on the Concept-ARC dataset, focusing on output-grid accuracy. It highlights a significant performance gap between textual and visual modalities, with Python tools improving visual accuracy. The analysis also notes that increased reasoning effort generally boosts textual accuracy, and that human performance on this task is lower than top reasoning models in the textual setting.",
          "entities": [
            "Concept-ARC",
            "OpenAI API",
            "Claude Sonnet",
            "Gemini 2.5 Pro",
            "Moskvichev et al."
          ],
          "keywords": [
            "output-grid accuracy",
            "reasoning models",
            "textual modality",
            "visual modality",
            "Python tools",
            "pass@1",
            "effort settings",
            "error-type distribution",
            "human-generated output grids"
          ],
          "key_points": [
            "Table 1 shows output-grid accuracy (pass@1) for various reasoning models across different effort settings and tool usage.",
            "Non-reasoning models have significantly lower accuracy than reasoning models.",
            "There is a substantial performance gap between textual and visual settings for all models.",
            "Enabling Python tools leads to a notable increase in visual accuracy, but not consistently in textual accuracy.",
            "Increased reasoning effort generally improves textual accuracy, and in the visual setting, it primarily leads to more Python code execution.",
            "Models struggle with recognizing correct grid sizes in visual settings, which Python tools partially address.",
            "Human-generated output grids achieved lower accuracy than top reasoning models in the textual modality."
          ],
          "technical_terms": [
            "output-grid accuracy",
            "pass@1",
            "textual modality",
            "visual modality",
            "reasoning models",
            "effort settings",
            "reasoning token budget",
            "temperature",
            "computer vision libraries",
            "ground-truth grids",
            "invalid outputs",
            "error-type distribution",
            "ARC-Prize evaluation"
          ],
          "status": "success"
        },
        {
          "page": 6,
          "section": "Introduction",
          "char_count": 5373,
          "text_preview": "Preprint. Under Review\n3.2 RULEEVALUATION\nOur team manually evaluated the rules generated by o3 in all settings and by Claude Sonnet 4 and\nGemini 2.5 Pro in the medium-effort + tools setting, for both textual and visual modalities. We also\nevaluated the pass@1 rules generated by humans, using data f...",
          "summary": "This section details the manual evaluation of rules generated by AI models (o3, Claude Sonnet 4, Gemini 2.5 Pro) and humans on ConceptARC tasks. The analysis reveals that while AI models can achieve high output accuracy, a significant portion of their correct outputs are based on unintended or incorrect rules, indicating a reliance on superficial patterns rather than true abstract reasoning. The study also explores the impact of reasoning effort and tool use on model performance, particularly highlighting the benefits of Python tool integration in the visual modality.",
          "entities": [
            "o3",
            "Claude Sonnet 4",
            "Gemini 2.5 Pro",
            "Moskvichev et al. (2023)",
            "ConceptARC tasks",
            "Chollet, 2024",
            "Hao et al., 2025"
          ],
          "keywords": [
            "rule evaluation",
            "abstract reasoning",
            "unintended rules",
            "output accuracy",
            "textual modality",
            "visual modality",
            "reasoning effort",
            "tool use",
            "pass@1",
            "human-generated rules"
          ],
          "key_points": [
            "Manual evaluation of AI-generated and human-generated rules for ConceptARC tasks was conducted.",
            "AI models often generate correct outputs based on unintended or incorrect rules, suggesting superficial pattern recognition.",
            "Human-generated rules show a lower percentage of correct outputs based on unintended/incorrect rules compared to AI.",
            "Models can recognize intended abstract rules but fail to apply them correctly to test grids.",
            "Increasing reasoning effort has a greater positive impact in the textual modality, while Python tool use significantly improves performance in the visual modality.",
            "Assessing AI capabilities solely by output accuracy can be misleading, especially in the visual domain."
          ],
          "technical_terms": [
            "pass@1",
            "correct-intended rules",
            "correct-unintended rules",
            "incorrect rules",
            "textual modality",
            "visual modality",
            "reasoning effort",
            "tool use",
            "output accuracy",
            "abstract concepts"
          ],
          "status": "success"
        },
        {
          "page": 7,
          "section": "Introduction",
          "char_count": 2298,
          "text_preview": "Preprint. Under Review\nTextualVisualHuman\nCorrectIncorrectCorrect IncorrectCorrectIncorrectCorrectIncorrectCorrect IncorrectCorrect IncorrectCorrectIncorrectGridGrid Grid Grid Grid Grid Grido3 Claude Gemini GeminiClaudeo3 Human\nFigure 2: Results of rule evaluations. For each model in each modality (...",
          "summary": "Page 7 discusses the results of rule evaluations for AI models (Claude, Gemini, o3, o4-mini) and humans on the ConceptARC dataset, presented in Figures 2 and 3. The discussion section begins to answer research questions by comparing AI model accuracy to human accuracy, noting that o3 with medium reasoning effort matches or surpasses human accuracy for textual inputs, while visual modality performance still lags significantly behind humans.",
          "entities": [
            "Claude",
            "Gemini",
            "o3",
            "o4-mini",
            "ConceptARC",
            "ARC-Prize",
            "Chollet et al., 2025",
            "Kamradt, 2025"
          ],
          "keywords": [
            "rule evaluations",
            "AI models",
            "human accuracy",
            "textual inputs",
            "visual modality",
            "ConceptARC tasks",
            "reasoning effort",
            "Python tools"
          ],
          "key_points": [
            "Figures 2 and 3 present results of rule evaluations for AI models and humans on ConceptARC tasks.",
            "AI models' performance is compared to human accuracy.",
            "o3 with medium reasoning effort matches or surpasses human accuracy for textual inputs.",
            "AI models' performance in the visual modality still lags significantly behind human accuracy.",
            "Python tools can improve performance for some models (e.g., o4-mini)."
          ],
          "technical_terms": [
            "rule evaluations",
            "modality",
            "textual inputs",
            "visual modality",
            "reasoning effort",
            "Python tools",
            "ConceptARC tasks",
            "correct-intended",
            "correct-unintended",
            "incorrect"
          ],
          "status": "success"
        },
        {
          "page": 8,
          "section": "Introduction",
          "char_count": 2316,
          "text_preview": "Preprint. Under Review\nModel RuleFind the value with the lowest density (actual positions / bounding box area), then create an output grid with dimensions equal to that value's bounding box, filled entirely with that valueTraining ExamplesTest IputGround TruthModel Output\nTraining ExamplesGround Tru...",
          "summary": "Page 8 discusses AI model rule generation for the ConceptARC dataset, highlighting how models can learn superficial shortcuts instead of intended abstractions. It presents examples of 'correct-unintended' rules generated by models like 'o3' and Claude Sonnet 4, which focus on shallow features or heuristics rather than deeper conceptual understanding. The text also poses a question about the extent to which AI-generated rules capture intended abstractions versus superficial shortcuts.",
          "entities": [
            "ConceptARC",
            "o3",
            "Claude Sonnet 4",
            "Python"
          ],
          "keywords": [
            "AI model rule generation",
            "ConceptARC",
            "intended abstractions",
            "superficial shortcuts",
            "correct-unintended rules",
            "density heuristic",
            "shallow inference",
            "overfitting",
            "reasoning effort"
          ],
          "key_points": [
            "AI models can generate rules that are correct for given test cases but do not capture the intended abstract concepts.",
            "Models may focus on shallow features or heuristics (e.g., density, color presence) rather than deeper relational understanding.",
            "Examples illustrate how models like 'o3' and Claude Sonnet 4 exhibit this behavior on ConceptARC tasks.",
            "The research questions the extent to which AI-generated rules capture intended abstractions versus superficial shortcuts."
          ],
          "technical_terms": [
            "bounding box",
            "density",
            "minimal bounding box",
            "heuristic",
            "shallow inference",
            "overfits",
            "3D stack"
          ],
          "status": "success"
        }
      ],
      "total_pages": 8,
      "total_chars": 29458,
      "total_entities": 68,
      "total_keywords": 74,
      "llm_successes": 8,
      "llm_failures": 0,
      "aggregate_summary": "This paper investigates the abstract reasoning capabilities of AI models, particularly OpenAI's o3-preview, using the ConceptARC benchmark. While some text-based models achieve human-level accuracy, their reasoning often relies on surface-level shortcuts rather than intended abstractions. Visual models show lower accuracy but a greater proportion of abstract rules, suggesting current evaluations may overestimate text-based and underestimate visual abstract reasoning. This section introduces the ..."
    }
  },
  "SM-21DA5C": {
    "status": "ok",
    "output": {
      "sm_id": "SM-21DA5C",
      "role": "Extract methodologies and key concepts from Related Work.",
      "assigned_sections": [
        "Related Work"
      ],
      "page_range": [
        9,
        15
      ],
      "results": [
        {
          "page": 9,
          "section": "Related Work",
          "char_count": 4612,
          "text_preview": "Preprint. Under Review\naccuracy) werecorrect and intended; that is, they captured the intended abstractions of the tasks.\nHowever, about 28% of o3’s generated rules werecorrect but unintended, meaning they were correct\nwith respect to the given demonstrations, and frequently generated correct output...",
          "summary": "This section discusses the limitations of AI models in abstract reasoning compared to humans, particularly in visual modalities. It highlights that AI models often rely on superficial features rather than intended abstractions, and that accuracy alone is an insufficient metric for evaluating abstract reasoning. The findings suggest directions for improving visual reasoning models and emphasize the importance of developing AI that can grasp human-like abstractions for better generalization and explainability.",
          "entities": [
            "ConceptARC",
            "ARC",
            "Chollet (2019)",
            "Claude",
            "Gemini",
            "Frank (2023)",
            "Ivanova (2025)",
            "Rane et al. (2025)",
            "ARC-Prize challenge"
          ],
          "keywords": [
            "abstract reasoning",
            "intended abstractions",
            "correct-unintended rules",
            "visual modalities",
            "textual modalities",
            "superficial features",
            "generalizable mechanisms",
            "accuracy",
            "robustness",
            "human-AI interaction"
          ],
          "key_points": [
            "AI models are more likely to miss intended abstractions and solve tasks using superficial features than humans.",
            "Output-grid and rule correctness drop dramatically in visual modes compared to textual modes.",
            "Reasoning effort is more helpful for textual inputs, while Python tools are more helpful for visual inputs.",
            "Accuracy alone can overestimate abstract reasoning capabilities in textual modalities and underestimate it in visual modalities.",
            "Evaluating robustness and generalizable mechanisms is crucial beyond simple accuracy.",
            "Developing AI models that grasp human-like abstractions is essential for generalization and explainability."
          ],
          "technical_terms": [
            "core knowledge priors",
            "objectness",
            "output grids",
            "rule correctness",
            "reasoning effort",
            "Python tools",
            "multimodal reasoning models",
            "rule shortcuts"
          ],
          "status": "success"
        },
        {
          "page": 10,
          "section": "Related Work",
          "char_count": 3265,
          "text_preview": "Preprint. Under Review\n• We cannot be certain that the natural-language rules generated by the AI models we evaluated\nare faithful representations of the actual reasoning the models do to solve a task, though in\ngeneral the output grids generated seem to align with the rules. Further study is needed...",
          "summary": "This section discusses limitations and considerations of the research, including the faithfulness of AI-generated rules, resource constraints affecting experiments, subjectivity in rule classification, and the use of pass@1 accuracy. It also addresses ethical considerations and reproducibility, noting the public availability of the ConceptARC dataset and potential non-determinism of AI models.",
          "entities": [
            "AI models",
            "o3",
            "Claude",
            "Gemini",
            "ARC-Prize",
            "Chollet (2024)",
            "Moskvichev et al. (2023)",
            "IRB exemption",
            "University of New Mexico IRB",
            "ConceptARC dataset",
            "OpenAI",
            "BANYAN project",
            "Sandia National Laboratories",
            "Templeton World Charity Foundation, Inc."
          ],
          "keywords": [
            "AI-generated rules",
            "reasoning faithfulness",
            "resource limitations",
            "rule classification",
            "pass@1 accuracy",
            "ConceptARC dataset",
            "reproducibility",
            "non-deterministic AI models",
            "reasoning traces",
            "Python calls"
          ],
          "key_points": [
            "Uncertainty about the faithfulness of AI-generated natural-language rules to actual model reasoning.",
            "Resource limitations prevented experimentation with 'high-effort' reasoning settings and larger token budgets.",
            "Manual classification of human- and machine-generated rules involved subjectivity, mitigated by team consensus.",
            "Pass@1 accuracies were used for both humans and machines, differing from other ARC evaluations.",
            "The prompt used was the same as in the ARC-Prize evaluation of o3 for the textual setting.",
            "Human-generated rule data was incomplete, lacking rules for incorrect outputs and some classifiable rules for correct outputs.",
            "Data used is from human studies with IRB exemption, containing no identifying information.",
            "Reproducibility may be affected by the non-deterministic nature of AI models (Temperature 1) and model releases/deprecations.",
            "Reasoning traces and Python calls were collected to document model outputs.",
            "The ConceptARC dataset is publicly available."
          ],
          "technical_terms": [
            "natural-language rules",
            "reasoning",
            "reasoning-token budgets",
            "pass@1 accuracies",
            "pass@2 accuracies",
            "pass@3 accuracies",
            "textual setting",
            "visual setting",
            "reasoning traces",
            "Temperature 1",
            "proprietary models"
          ],
          "status": "success"
        },
        {
          "page": 11,
          "section": "Related Work",
          "char_count": 3043,
          "text_preview": "Preprint. Under Review\nREFERENCES\nARC-Prize. ARC-AGI benchmarking, 2024. https://github.com/arcprize/\narc-agi-benchmarking.\nARC-Prize. ARC-AGI leaderboard, 2025.https://arcprize.org/leaderboard.\nSusan Carey.The Origin of Concepts. MIT Press, Cambridge, MA, 2011.\nFrançois Chollet. On the measure of i...",
          "summary": "This page lists references for a research paper, primarily focusing on the Abstraction and Reasoning Corpus (ARC) and related benchmarks. It includes works on evaluating AI reasoning, concept formation, and shortcut learning in large language models.",
          "entities": [
            "ARC-Prize",
            "Susan Carey",
            "François Chollet",
            "Mike Knoop",
            "Gregory Kamradt",
            "Bryan Landers",
            "Henry Pinkard",
            "Mengnan Du",
            "Fengxiang He",
            "Na Zou",
            "Dacheng Tao",
            "Xia Hu",
            "Harry E. Foundalis",
            "Michael C. Frank",
            "Robert Geirhos",
            "Jörn-Henrik Jacobsen",
            "Claudio Michaelis",
            "Richard Zemel",
            "Wieland Brendel",
            "Matthias Bethge",
            "Felix A. Wichmann",
            "Yunzhuo Hao",
            "Jiawei Gu",
            "Huichen Will Wang",
            "Linjie Li",
            "Zhengyuan Yang",
            "Lijuan Wang",
            "Yu Cheng",
            "Douglas R. Hofstadter",
            "Dedre Gentner",
            "Keith J. Holyoak",
            "Boicho N. Kokinov",
            "Anna A. Ivanova",
            "Brenden M. Lake",
            "Tomer D. Ullman",
            "Joshua J. Tenenbaum",
            "Samuel J. Gershman",
            "Solim LeGris",
            "Wai Keen Vong",
            "Todd M. Gureckis",
            "Arseny Moskvichev",
            "Victor Vikram Odouard",
            "Melanie Mitchell",
            "OpenAI",
            "Abstraction and Reasoning Corpus (ARC)",
            "ARC-AGI benchmarking",
            "ARC-AGI leaderboard",
            "ARC-AGI-Pub",
            "ARC Prize 2024: Technical Report",
            "ARC-AGI-2",
            "Bongard Problems",
            "ConceptARC benchmark",
            "EMMA: An enhanced multimodal reasoning benchmark",
            "Thinking With Images"
          ],
          "keywords": [
            "Abstraction and Reasoning Corpus (ARC)",
            "ARC-AGI",
            "reasoning systems",
            "benchmarking",
            "large language models",
            "concept formation",
            "shortcut learning",
            "multimodal reasoning",
            "cognitive abilities",
            "human performance"
          ],
          "key_points": [
            "The Abstraction and Reasoning Corpus (ARC) and its associated benchmarks (ARC-AGI, ARC-AGI-2) are central to evaluating AI reasoning capabilities.",
            "Several works focus on evaluating the cognitive abilities and reasoning capacities of large language models (LLMs).",
            "The concept of 'shortcut learning' in LLMs is discussed as a potential pitfall in natural language understanding.",
            "Research explores multimodal reasoning and the development of benchmarks for such tasks.",
            "The origin of concepts and analogy as a core mechanism of thought are foundational theoretical underpinnings."
          ],
          "technical_terms": [
            "Abstraction and Reasoning Corpus (ARC)",
            "ARC-AGI",
            "large language models (LLMs)",
            "multimodal reasoning",
            "shortcut learning",
            "benchmarking",
            "generalization",
            "cognitive abilities",
            "deep neural networks"
          ],
          "status": "success"
        },
        {
          "page": 12,
          "section": "Related Work",
          "char_count": 543,
          "text_preview": "Preprint. Under Review\nSunayana Rane, Cyrus Kirkman, Amanda Royka, Graham Todd, Ryan Law, Jacob Gates Foster, and\nErica Cartmill. Principles of animal cognition for LLM evaluations: A case study on transitive\ninference. InProceedings of the International Conference on Machine Learning, ICML-2025,\n20...",
          "summary": "This page lists two research papers relevant to the current work. The first paper, by Rane et al., proposes principles of animal cognition for evaluating Large Language Models (LLMs), using transitive inference as a case study. The second paper, by Zhang et al., introduces RA VEN, a dataset for relational and analogical visual reasoning.",
          "entities": [
            "Sunayana Rane",
            "Cyrus Kirkman",
            "Amanda Royka",
            "Graham Todd",
            "Ryan Law",
            "Jacob Gates Foster",
            "Erica Cartmill",
            "Chi Zhang",
            "Feng Gao",
            "Baoxiong Jia",
            "Yixin Zhu",
            "Song-Chun Zhu",
            "ICML-2025",
            "IEEE/CVF Conference on Computer Vision and Pattern Recognition",
            "RA VEN"
          ],
          "keywords": [
            "animal cognition",
            "LLM evaluations",
            "transitive inference",
            "relational reasoning",
            "analogical reasoning",
            "visual reasoning",
            "dataset"
          ],
          "key_points": [
            "Rane et al. (2025) apply animal cognition principles to LLM evaluations, focusing on transitive inference.",
            "Zhang et al. (2019) present RA VEN, a dataset for relational and analogical visual reasoning."
          ],
          "technical_terms": [
            "Large Language Models (LLMs)",
            "transitive inference",
            "relational reasoning",
            "analogical reasoning",
            "visual reasoning"
          ],
          "status": "success"
        },
        {
          "page": 13,
          "section": "Related Work",
          "char_count": 1463,
          "text_preview": "Preprint. Under Review\nA TEXTUALPROMPT\nFind the common rule that maps an input grid to an output grid, given the examples below.\nExample 1\nInput:\n0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 4 4 4 4 0 0 0 0 0\n0 0 0 4 4 4 4 0 0 0 0 0\n0 0 0 4 4 4 4 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0\n2 2 2 2 2 ...",
          "summary": "This page presents a task from a research paper focused on identifying a common rule that transforms an input grid into an output grid, based on provided examples. It includes two variants of the task: 'No Tools Variant' and 'Tools Variant', and a test input grid for prediction.",
          "entities": [],
          "keywords": [
            "input grid",
            "output grid",
            "common rule",
            "transformation rule",
            "examples",
            "test input grid",
            "No Tools Variant",
            "Tools Variant"
          ],
          "key_points": [
            "The core task is to find a rule that maps an input grid to an output grid.",
            "Examples are provided to illustrate the transformation.",
            "Two variants of the task are presented: one without tools and one with potential tool usage.",
            "A test input grid is given for prediction based on the discovered rule."
          ],
          "technical_terms": [
            "input grid",
            "output grid",
            "transformation rule"
          ],
          "status": "success"
        },
        {
          "page": 14,
          "section": "Related Work",
          "char_count": 1200,
          "text_preview": "Preprint. Under Review\nB VISUALPROMPT\nThe left side of the first image shows 3 grids, where each grid square is colored with one of\n10 possible colors: black, blue, red, green, yellow, gray, magenta, orange, cyan or brown. The\nright side of the first image also contains 3 grids, each of which is a t...",
          "summary": "This section describes a visual reasoning task called VISUALPROMPT. It involves inferring a transformation rule from example grids and applying it to a new grid. The task has two variants: 'No Tools' and 'Tools', with the latter allowing Python for problem-solving.",
          "entities": [
            "VISUALPROMPT"
          ],
          "keywords": [
            "visual reasoning",
            "transformation rule",
            "grid manipulation",
            "pattern recognition",
            "inference",
            "VISUALPROMPT",
            "No Tools Variant",
            "Tools Variant"
          ],
          "key_points": [
            "The task requires determining a single rule that transforms grids.",
            "The rule is applied to a new grid after being inferred from examples.",
            "Two variants exist: one without external tools and one allowing Python.",
            "The output can be described using natural language with color indices."
          ],
          "technical_terms": [
            "transformation rule",
            "grid",
            "color indices"
          ],
          "status": "success"
        },
        {
          "page": 15,
          "section": "Related Work",
          "char_count": 1843,
          "text_preview": "Preprint. Under Review\nC PROMPTS FORNON-REASONING MODELS\nThe prompts we used for non-Reasoning models were minimally modified to require an additional\nfield containing a reasoning trace in the final JSON object. Otherwise, the prompts were consistent\nwith those used for reasoning models, including v...",
          "summary": "This page details modifications made to prompts for non-reasoning models, requiring an additional field for a reasoning trace in the JSON output. It also presents data for rule evaluation plots (Figure 2), showing the breakdown of task classifications (Correct-Intended, Correct-Unintended, Incorrect) for different models (o3, Claude Sonnet 4, Gemini 2.5 Pro) and human-generated rules, partitioned by modality (Textual vs. Visual) and output grid correctness.",
          "entities": [
            "o3",
            "Claude Sonnet 4",
            "Gemini 2.5 Pro"
          ],
          "keywords": [
            "non-reasoning models",
            "reasoning trace",
            "JSON object",
            "rule evaluation plots",
            "task classification",
            "modality",
            "output grid correctness",
            "human-generated rules"
          ],
          "key_points": [
            "Prompts for non-reasoning models were modified to include a reasoning trace field in the JSON output.",
            "The data presented in Table 2 supports Figure 2 for rule evaluation plots.",
            "The table categorizes task performance by rule classification (Correct-Intended, Correct-Unintended, Incorrect), modality (Textual vs. Visual), and output grid correctness (Correct Grid vs. Incorrect Grid).",
            "Performance data is provided for models o3, Claude Sonnet 4, Gemini 2.5 Pro, and human-generated rules."
          ],
          "technical_terms": [
            "non-Reasoning models",
            "reasoning trace",
            "JSON object",
            "rule evaluation plots",
            "rule classification",
            "Correct-Intended",
            "Correct-Unintended",
            "Incorrect",
            "modality",
            "Textual",
            "Visual",
            "Correct Grid",
            "Incorrect Grid",
            "grid accuracy",
            "Not Classified"
          ],
          "status": "success"
        }
      ],
      "total_pages": 7,
      "total_chars": 15969,
      "total_entities": 96,
      "total_keywords": 61,
      "llm_successes": 7,
      "llm_failures": 0,
      "aggregate_summary": "This section discusses the limitations of AI models in abstract reasoning compared to humans, particularly in visual modalities. It highlights that AI models often rely on superficial features rather than intended abstractions, and that accuracy alone is an insufficient metric for evaluating abstract reasoning. The findings suggest directions for improving visual reasoning models and emphasize the importance of developing AI that can grasp human-like abstractions for better generalization and ex..."
    }
  },
  "SM-3F0DDB": {
    "status": "ok",
    "output": {
      "sm_id": "SM-3F0DDB",
      "role": "Extract key findings and methodologies across the entire document.",
      "assigned_sections": [
        "Abstract",
        "Introduction",
        "Related Work"
      ],
      "page_range": [
        1,
        15
      ],
      "results": [
        {
          "page": 1,
          "section": "Abstract",
          "char_count": 3336,
          "text_preview": "Preprint. Under Review\nDOAI MODELSPERFORMHUMAN-LIKEABSTRACT\nREASONINGACROSSMODALITIES?\nClaas Beger\nSanta Fe Institute\nclaasbeger@santafe.edu\nRyan Yi\nSanta Fe Institute\nryi@santafe.edu\nShuhao Fu\nSanta Fe Institute\nfushuhao@santafe.edu\nArseny Moskvichev\nAdvanced Micro Devices, Inc.\narseny.moskvichev@g...",
          "summary": "This paper investigates the abstraction abilities of AI models, particularly OpenAI's o3-preview, using the ConceptARC benchmark. It evaluates models across different input modalities (textual vs. visual) and tool usage, assessing both output accuracy and the natural-language rules generated to explain solutions. The findings suggest that while text-based models can match human accuracy, their reasoning often relies on surface-level patterns, overestimating their abstract reasoning capabilities. Visual models show lower accuracy but potentially better underlying abstraction, though they struggle with rule application.",
          "entities": [
            "OpenAI",
            "o3-preview",
            "ARC-AGI benchmark",
            "ConceptARC benchmark",
            "Santa Fe Institute",
            "Advanced Micro Devices, Inc.",
            "Sandia National Laboratories",
            "Claas Beger",
            "Ryan Yi",
            "Shuhao Fu",
            "Arseny Moskvichev",
            "Sarah W. Tsai",
            "Sivasankaran Rajamanickam",
            "Melanie Mitchell",
            "Abstraction and Reasoning Corpus (ARC)",
            "Chollet"
          ],
          "keywords": [
            "abstract reasoning",
            "AI models",
            "ConceptARC benchmark",
            "multimodal",
            "rule induction",
            "analogical reasoning",
            "surface-level patterns",
            "generalization",
            "human-like intelligence"
          ],
          "key_points": [
            "Investigates abstraction abilities of AI models using the ConceptARC benchmark.",
            "Evaluates models based on input modality (textual vs. visual), external tool usage, and reasoning effort.",
            "Performs fine-grained evaluation of natural-language rules generated by models to explain solutions, in addition to output accuracy.",
            "Results show text-based models can match human accuracy but often rely on surface-level shortcuts, overestimating abstract reasoning.",
            "Visual models have lower accuracy but may exhibit more intended abstractions, though they struggle with rule application.",
            "Accuracy alone may overestimate abstract reasoning in textual modalities and underestimate it in visual modalities.",
            "The proposed evaluation framework offers a more faithful picture of multimodal models' abstract reasoning abilities."
          ],
          "technical_terms": [
            "abstraction",
            "reasoning",
            "modality",
            "ARC-AGI benchmark",
            "ConceptARC benchmark",
            "external Python tools",
            "reasoning effort",
            "natural-language rules",
            "surface-level patterns",
            "few-shot rule-induction",
            "analogical reasoning",
            "grids",
            "cells",
            "colors",
            "multimodal models"
          ],
          "status": "success"
        },
        {
          "page": 2,
          "section": "Introduction",
          "char_count": 4750,
          "text_preview": "Preprint. Under Review\nTo solve a task, an agent should infer a rule governing the demonstrations and apply that rule to the\ntest input to produce a correct output grid.\nChollet 2025 devised 1,000 such tasks, releasing 400 easier puzzles as a “training set,” 400 harder\npuzzles as an “evaluation set,...",
          "summary": "This section introduces the Abstract Reasoning Corpus (ARC) and its challenges, highlighting the performance of OpenAI's o3 model which achieved significant accuracy but raises questions about the nature of its abstract reasoning. The authors propose using the ConceptARC benchmark to investigate whether AI systems infer generalizable abstractions or rely on superficial patterns, and plan to assess this across different modalities, reasoning efforts, and tool access.",
          "entities": [
            "Chollet",
            "OpenAI",
            "o3 model",
            "ARC-AGI Prize competition",
            "ConceptARC",
            "Moskvichev et al."
          ],
          "keywords": [
            "abstract reasoning",
            "ARC tasks",
            "ConceptARC benchmark",
            "generalizable abstractions",
            "superficial patterns",
            "modality",
            "reasoning effort",
            "Python tool access"
          ],
          "key_points": [
            "An agent infers a rule from demonstrations to solve a task by applying it to a test input.",
            "Chollet devised 1,000 ARC tasks, split into training, evaluation, and private test sets.",
            "The 2024 ARC-AGI Prize competition aimed for programs exceeding 85% accuracy on private test sets.",
            "The top scoring program in the competition achieved about 54% accuracy using a fine-tuned LLM and data augmentation.",
            "OpenAI's o3 model achieved 76% accuracy (low-effort) and 88% accuracy (high-effort) on a semi-private test set.",
            "The performance of o3 was described as a 'genuine breakthrough' for AI capabilities.",
            "It is unclear if AI systems like o3 solve ARC tasks using intended generalizable abstractions or infer 'shortcuts' based on unintended correlations.",
            "The study assesses abstractions used by commercial and open-weight models on ConceptARC, a benchmark focused on basic spatial and semantic concepts.",
            "ConceptARC is designed to test robust understanding of concepts by varying contexts and requiring generalization.",
            "Previous evaluations used text-based representations of grids; this study investigates both textual and visual modalities.",
            "The research will examine how reasoning effort (token budget) and access to external tools (Python code) affect model performance and abstraction discovery.",
            "The study aims to answer: (1) AI vs. human accuracy on ConceptARC, (2) extent of intended vs. superficial patterns used by AI and humans, and (3) impact of modality, reasoning effort, and tool access on solving tasks via intended abstractions."
          ],
          "technical_terms": [
            "agent",
            "rule inference",
            "output grid",
            "LLM",
            "data augmentation",
            "accuracy",
            "private test set",
            "semi-private test set",
            "low-effort setting",
            "high-effort setting",
            "computing cost",
            "abstract reasoning abilities",
            "generalize",
            "abstract concepts",
            "instantiations",
            "shortcuts",
            "unintended correlations",
            "commercial models",
            "open-weight models",
            "ConceptARC",
            "ARC domain",
            "spatial concepts",
            "semantic concepts",
            "generalization",
            "robust understanding",
            "text-based representations",
            "integer matrix",
            "visual modalities",
            "reasoning effort",
            "token budget",
            "external tools",
            "Python code"
          ],
          "status": "success"
        },
        {
          "page": 3,
          "section": "Introduction",
          "char_count": 2914,
          "text_preview": "Preprint. Under Review\nFigure 1: Each row shows a task from the ConceptARC benchmark. Each task shown consists of\nthree demonstrations of a transformation and one test grid. In this study, the solver is tasked with\ngenerating a rule that describes the transformations and applying that rule to the te...",
          "summary": "This section introduces the methodology for evaluating AI models on the ConceptARC benchmark. It details the dataset's creation, focusing on 16 basic spatial and semantic concepts with 480 tasks designed to be human-solvable. The study evaluates several proprietary multimodal \"reasoning\" models alongside non-reasoning models, using both textual and visual modalities, and assesses performance based on grid output accuracy and rule abstraction capture.",
          "entities": [
            "ConceptARC benchmark",
            "Moskvichev et al. 2023",
            "OpenAI",
            "o3",
            "o4-mini",
            "Google",
            "Gemini 2.5 Pro",
            "Anthropic",
            "Claude Sonnet 4",
            "GPT-4o",
            "Meta",
            "Llama 4 Scout",
            "Alibaba",
            "Qwen 2.5 VL 72B",
            "Chollet et al.’s 2024",
            "Prolific Academic platform"
          ],
          "keywords": [
            "ConceptARC benchmark",
            "multimodal reasoning models",
            "AI model evaluation",
            "spatial and semantic concepts",
            "transformation rule",
            "grid output accuracy",
            "rule abstraction capture",
            "pass@1 results",
            "textual modality",
            "visual modality"
          ],
          "key_points": [
            "ConceptARC dataset comprises 480 tasks based on 16 basic spatial and semantic concepts, designed to be human-solvable.",
            "Four proprietary multimodal \"reasoning\" models (OpenAI o3, o4-mini, Google Gemini 2.5 Pro, Anthropic Claude Sonnet 4) and three non-reasoning models (OpenAI GPT-4o, Meta Llama 4 Scout, Alibaba Qwen 2.5 VL 72B) were evaluated.",
            "Models were evaluated using both textual and visual modalities with specific prompts.",
            "Evaluation criteria include grid output accuracy and the degree to which generated rules capture task abstractions.",
            "Human-generated solutions were also evaluated using the same criteria.",
            "pass@1 results are reported for both AI models and humans due to resource constraints."
          ],
          "technical_terms": [
            "multimodal reasoning models",
            "non-reasoning multimodal models",
            "temperature 0",
            "temperature 1.0",
            "textual modality",
            "visual modality",
            "JSON object",
            "matrix of integers",
            "grid output accuracy",
            "rule abstraction capture",
            "pass@1 results",
            "context window"
          ],
          "status": "success"
        },
        {
          "page": 4,
          "section": "Introduction",
          "char_count": 4904,
          "text_preview": "Preprint. Under Review\nWe evaluated o3 under its low- and medium-effort reasoning settings.5 We evaluated Gemini 2.5\nPro and Claude Sonnet 4 with a reasoning budget of 16,000 tokens, chosen to roughly approximate\nOpenAI’s medium-effort setting. Additionally, across all reasoning models and modalitie...",
          "summary": "This section details the evaluation methodology for AI models on the ConceptARC dataset. It describes how output-grid accuracy is assessed and introduces a novel approach of evaluating natural-language rules generated by models to understand their grasp of abstract concepts versus superficial pattern exploitation. The section also provides examples of human and model-generated rules and their classifications.",
          "entities": [
            "o3",
            "Gemini 2.5 Pro",
            "Claude Sonnet 4",
            "OpenAI",
            "ConceptARC corpus",
            "Du et al., 2023",
            "Geirhos et al., 2020",
            "Moskvichev et al. 2023"
          ],
          "keywords": [
            "output-grid accuracy",
            "natural-language rule",
            "abstract concepts",
            "superficial patterns",
            "shortcuts",
            "human judgment",
            "rule annotation",
            "correct-intended",
            "correct-unintended",
            "reasoning settings"
          ],
          "key_points": [
            "Output-grid accuracy is evaluated by comparing model-generated grids to ground truth solutions in the ConceptARC corpus, requiring exact matches.",
            "The study investigates whether output-grid correctness reflects understanding of abstract concepts or exploitation of superficial patterns.",
            "Models are asked to output both a transformed test grid and a natural-language rule describing the transformation.",
            "Natural-language rules are manually annotated as 'incorrect', 'correct-unintended', or 'correct-intended'.",
            "Examples of human and model-generated rules are provided with their respective annotations.",
            "The study notes that o3, when given textual input, often phrases rules in terms of colors or pixels rather than objects."
          ],
          "technical_terms": [
            "reasoning settings",
            "reasoning budget",
            "token budget",
            "modalities",
            "tool-access conditions",
            "Python tools",
            "output-grid accuracy",
            "ground-truth solution",
            "ConceptARC corpus",
            "demonstrations",
            "test grid images",
            "editing tool",
            "matrix",
            "colors encoded as integers",
            "exact matches",
            "AI model performance",
            "abstract concepts",
            "superficial patterns",
            "shortcuts",
            "large neural-network models",
            "spurious patterns",
            "natural-language rule",
            "human participants",
            "human judgment",
            "manually annotated",
            "incorrect",
            "correct-unintended",
            "correct-intended",
            "intended abstractions",
            "textual inputs",
            "medium reasoning effort",
            "visual input",
            "textual representation",
            "bounding box",
            "LLMs"
          ],
          "status": "success"
        },
        {
          "page": 5,
          "section": "Introduction",
          "char_count": 3567,
          "text_preview": "Preprint. Under Review\nTable 1:Reasoning models:Output-grid accuracy (pass@1) for Concept-ARC across models and\nexperimental settings. Accuracy is shown in %. Each cell showstextual / visualaccuracy. For o3 and\no4-mini, we use the “low” and “medium” effort settings in the OpenAI API. For Claude and ...",
          "summary": "Page 5 presents results on output-grid accuracy for various reasoning models on the Concept-ARC dataset. It highlights a significant performance gap between textual and visual modalities, with reasoning models generally outperforming non-reasoning models. The use of Python tools substantially improves visual accuracy, and increased reasoning effort boosts textual accuracy for some models.",
          "entities": [
            "Concept-ARC",
            "OpenAI API",
            "Claude Sonnet",
            "Gemini 2.5 Pro",
            "Moskvichev et al.",
            "ARC-Prize"
          ],
          "keywords": [
            "output-grid accuracy",
            "reasoning models",
            "textual modality",
            "visual modality",
            "Python tools",
            "pass@1",
            "effort settings",
            "failure cases"
          ],
          "key_points": [
            "Non-reasoning models attain much lower accuracy than reasoning models.",
            "There is a dramatic performance gap between the textual and visual settings for all models.",
            "Visual accuracy significantly improves when Python tools are enabled, especially for o3 and o4-mini.",
            "Python tools do not have a similar effect in the textual setting for most models.",
            "Increased reasoning effort is associated with increased accuracy in the textual modality.",
            "Models struggle to recognize correct grid size from image inputs in the visual setting.",
            "Human-generated output grids achieved 73% pass@1 accuracy on Concept-ARC tasks, lower than top reasoning models in textual modality.",
            "Strict evaluation (perfect match and requested format) is used for accuracy counting."
          ],
          "technical_terms": [
            "output-grid accuracy",
            "pass@1",
            "textual accuracy",
            "visual accuracy",
            "reasoning models",
            "non-reasoning models",
            "effort settings",
            "reasoning token budget",
            "temperature",
            "Python tools",
            "computer vision libraries",
            "ground-truth grids",
            "invalid outputs",
            "error-type distribution"
          ],
          "status": "success"
        },
        {
          "page": 6,
          "section": "Introduction",
          "char_count": 5373,
          "text_preview": "Preprint. Under Review\n3.2 RULEEVALUATION\nOur team manually evaluated the rules generated by o3 in all settings and by Claude Sonnet 4 and\nGemini 2.5 Pro in the medium-effort + tools setting, for both textual and visual modalities. We also\nevaluated the pass@1 rules generated by humans, using data f...",
          "summary": "This section details the manual evaluation of rules generated by AI models (o3, Claude Sonnet 4, Gemini 2.5 Pro) and humans on ConceptARC tasks. The evaluation goes beyond simple output accuracy to assess the correctness and intendedness of the underlying rules, revealing that AI models, particularly o3, often rely on superficial patterns rather than abstract concepts. The impact of reasoning effort and tool use on model performance is also investigated.",
          "entities": [
            "o3",
            "Claude Sonnet 4",
            "Gemini 2.5 Pro",
            "Moskvichev et al. (2023)",
            "ConceptARC tasks",
            "Chollet (2024)",
            "Hao et al. (2025)"
          ],
          "keywords": [
            "rule evaluation",
            "abstract reasoning",
            "unintended rules",
            "output accuracy",
            "reasoning effort",
            "tool use",
            "textual modality",
            "visual modality",
            "pass@1"
          ],
          "key_points": [
            "Manual evaluation of generated rules by o3, Claude, and Gemini, as well as human-generated rules, was conducted for textual and visual modalities.",
            "Evaluations assessed rule correctness (correct-intended, correct-unintended, incorrect) and output grid accuracy.",
            "o3 in the textual setting rivals humans in output grid accuracy, but a significant portion of its correct outputs are based on unintended or incorrect rules.",
            "AI models often generate rules based on superficial patterns or irrelevant features, unlike humans who better grasp intended abstractions.",
            "Incorrect output grids can sometimes be based on correct-intended rules, indicating a failure in applying the rule rather than understanding it.",
            "In the textual setting, increasing reasoning effort positively impacts accuracy and rule correctness more than tool use.",
            "In the visual setting, enabling Python tool use substantially improves output accuracy and rule correctness, especially at medium reasoning effort.",
            "Test-time scaling (reasoning effort) has less effect in visual modalities compared to text-only LLMs."
          ],
          "technical_terms": [
            "pass@1",
            "medium-effort + tools",
            "textual modality",
            "visual modality",
            "Correct Grid",
            "Incorrect Grid",
            "correct-intended",
            "correct-unintended",
            "incorrect rules",
            "output accuracy",
            "reasoning effort",
            "Python tool use",
            "LLMs"
          ],
          "status": "success"
        },
        {
          "page": 7,
          "section": "Introduction",
          "char_count": 2298,
          "text_preview": "Preprint. Under Review\nTextualVisualHuman\nCorrectIncorrectCorrect IncorrectCorrectIncorrectCorrectIncorrectCorrect IncorrectCorrect IncorrectCorrectIncorrectGridGrid Grid Grid Grid Grid Grido3 Claude Gemini GeminiClaudeo3 Human\nFigure 2: Results of rule evaluations. For each model in each modality (...",
          "summary": "Page 7 presents results from rule evaluations for AI models (Claude, Gemini, o3, o4-mini) and humans across textual and visual modalities on ConceptARC tasks. It highlights that o3 with medium reasoning effort matches or surpasses human accuracy for textual inputs, while visual modality performance still lags significantly behind humans for all models, even with tools.",
          "entities": [
            "Claude",
            "Gemini",
            "o3",
            "o4-mini",
            "ConceptARC",
            "ARC-Prize",
            "Chollet et al.",
            "Kamradt"
          ],
          "keywords": [
            "rule evaluations",
            "AI models",
            "human accuracy",
            "textual inputs",
            "visual modality",
            "ConceptARC tasks",
            "Python tools",
            "medium reasoning effort"
          ],
          "key_points": [
            "AI models' accuracy is compared to human accuracy on ConceptARC tasks.",
            "For textual inputs, o3 with medium reasoning effort matches or surpasses human accuracy.",
            "Claude and Gemini have lower accuracy than o3 for textual inputs.",
            "o4-mini surpasses humans only when Python tools are enabled for textual inputs.",
            "In the visual modality, AI models' performance significantly lags behind human accuracy, even with Python tools.",
            "Figures 2 and 3 show results of rule evaluations for different models and settings.",
            "Appendix D contains actual percentages corresponding to bar regions in the figures."
          ],
          "technical_terms": [
            "rule evaluations",
            "modality",
            "ConceptARC tasks",
            "correct-intended",
            "correct-unintended",
            "incorrect",
            "textual inputs",
            "visual modality",
            "reasoning effort",
            "Python tools",
            "ARC-AGI-1",
            "o3-preview"
          ],
          "status": "success"
        },
        {
          "page": 8,
          "section": "Introduction",
          "char_count": 2316,
          "text_preview": "Preprint. Under Review\nModel RuleFind the value with the lowest density (actual positions / bounding box area), then create an output grid with dimensions equal to that value's bounding box, filled entirely with that valueTraining ExamplesTest IputGround TruthModel Output\nTraining ExamplesGround Tru...",
          "summary": "Page 8 presents examples of \"correct-unintended\" rules generated by AI models on the ConceptARC dataset. It highlights how models like 'o3' and 'Claude Sonnet 4' employ shallow inference, heuristics, or overfitting to training data, rather than capturing the intended abstract concepts. The text also begins to quantify the extent to which AI-generated rules align with intended abstractions versus superficial shortcuts.",
          "entities": [
            "ConceptARC",
            "o3",
            "Claude Sonnet 4"
          ],
          "keywords": [
            "correct-unintended rules",
            "shallow inference",
            "overfitting",
            "heuristics",
            "abstract concepts",
            "ConceptARC dataset",
            "AI models",
            "superficial shortcuts"
          ],
          "key_points": [
            "AI models can generate rules that are correct for specific test cases but do not capture the intended abstract concepts.",
            "Examples illustrate models focusing on superficial features like pixel presence or density instead of relational understanding.",
            "Model 'o3' exhibits shallow inference and overfitting to training examples.",
            "Model 'Claude Sonnet 4' uses a density heuristic that fails to capture the intended concept of the bottommost shape.",
            "The research investigates the extent to which AI-generated rules capture intended abstractions versus superficial shortcuts."
          ],
          "technical_terms": [
            "bounding box",
            "density",
            "minimal bounding box",
            "shallow inference",
            "overfits",
            "density heuristic",
            "3D stack",
            "textual inputs",
            "reasoning effort",
            "Python tools"
          ],
          "status": "success"
        },
        {
          "page": 9,
          "section": "Related Work",
          "char_count": 4612,
          "text_preview": "Preprint. Under Review\naccuracy) werecorrect and intended; that is, they captured the intended abstractions of the tasks.\nHowever, about 28% of o3’s generated rules werecorrect but unintended, meaning they were correct\nwith respect to the given demonstrations, and frequently generated correct output...",
          "summary": "This section discusses the limitations of AI models in abstract reasoning compared to humans, particularly in visual modalities. It highlights that AI models often rely on superficial features rather than intended abstractions, leading to an overestimation of capabilities when using accuracy alone. The findings suggest directions for improving visual reasoning and emphasize the need for more robust evaluation metrics beyond simple accuracy.",
          "entities": [
            "ConceptARC",
            "ARC",
            "Chollet (2019)",
            "Claude",
            "Gemini",
            "Frank (2023)",
            "Ivanova (2025)",
            "Rane et al. (2025)",
            "ARC-Prize challenge"
          ],
          "keywords": [
            "abstract reasoning",
            "intended abstractions",
            "correct-unintended rules",
            "visual modalities",
            "textual modalities",
            "superficial features",
            "accuracy",
            "robustness",
            "generalizable mechanisms",
            "human-AI interaction"
          ],
          "key_points": [
            "AI models are more likely to miss intended abstractions and solve tasks using superficial features than humans.",
            "Visual modalities significantly decrease output-grid and rule correctness compared to textual modalities.",
            "Models are better at forming correct-intended rules than generating correct output grids in visual mode.",
            "Reasoning effort is more helpful for textual inputs, while Python tools are more helpful for visual inputs.",
            "Accuracy alone can overestimate abstract-reasoning capabilities in textual modalities and underestimate it in visual modalities.",
            "Evaluating abstract reasoning requires assessing robustness and the use of generalizable mechanisms over superficial shortcuts.",
            "AI models still lag humans in abstract reasoning, especially in visual reasoning abilities.",
            "Improving abstraction capabilities is essential for AI generalization, trustworthiness, and human-understandable reasoning."
          ],
          "technical_terms": [
            "abstract reasoning",
            "core knowledge priors",
            "objectness",
            "visual modalities",
            "textual modalities",
            "output grids",
            "rule correctness",
            "reasoning effort",
            "Python tools",
            "multimodal reasoning",
            "generalizable mechanisms",
            "superficial shortcuts"
          ],
          "status": "success"
        },
        {
          "page": 10,
          "section": "Related Work",
          "char_count": 3265,
          "text_preview": "Preprint. Under Review\n• We cannot be certain that the natural-language rules generated by the AI models we evaluated\nare faithful representations of the actual reasoning the models do to solve a task, though in\ngeneral the output grids generated seem to align with the rules. Further study is needed...",
          "summary": "This page discusses limitations and potential future work for the research. It highlights uncertainties in AI-generated rule faithfulness, resource constraints affecting experimental scope, subjectivity in rule classification, and differences in evaluation metrics compared to prior work. The page also includes statements on ethics, reproducibility, and acknowledgments.",
          "entities": [
            "AI models",
            "Claude",
            "Gemini",
            "o3",
            "ARC-Prize",
            "Chollet (2024)",
            "Moskvichev et al. (2023)",
            "IRB exemption",
            "University of New Mexico IRB",
            "ConceptARC dataset",
            "OpenAI",
            "BANYAN project",
            "Sandia National Laboratories",
            "Templeton World Charity Foundation, Inc."
          ],
          "keywords": [
            "AI-generated rules",
            "reasoning faithfulness",
            "resource limitations",
            "rule classification",
            "pass@1 accuracy",
            "reproducibility",
            "ethics statement",
            "ConceptARC dataset",
            "reasoning traces",
            "proprietary models"
          ],
          "key_points": [
            "Uncertainty exists regarding the faithfulness of AI-generated natural-language rules to the models' actual reasoning processes.",
            "Resource limitations prevented experiments with 'high-effort' reasoning settings or larger token budgets, which could have improved rule correctness and accuracy.",
            "Human classification of rules involved subjectivity, though consensus was reached to mitigate individual bias.",
            "The study used pass@1 accuracies, differing from pass@2 and pass@3 used in other ARC evaluations.",
            "The prompt used was similar to a previous ARC-Prize evaluation, and alternative prompts might yield different results.",
            "Human-generated rule data was incomplete, lacking rules for incorrect outputs and some unclassifiable rules for correct outputs.",
            "The study obtained IRB exemption and used anonymized data, with no other identified ethical issues.",
            "Reproducibility is planned via a public webpage with data and code, though AI model non-determinism and model release unpredictability are noted as potential challenges.",
            "Reasoning traces and Python calls were collected to document model outputs."
          ],
          "technical_terms": [
            "natural-language rules",
            "reasoning",
            "output grids",
            "reasoning-token budgets",
            "pass@1 accuracies",
            "pass@2 accuracies",
            "pass@3 accuracies",
            "textual setting",
            "visual setting",
            "reasoning traces",
            "Temperature 1",
            "proprietary models",
            "derived rules"
          ],
          "status": "success"
        },
        {
          "page": 11,
          "section": "Related Work",
          "char_count": 3043,
          "text_preview": "Preprint. Under Review\nREFERENCES\nARC-Prize. ARC-AGI benchmarking, 2024. https://github.com/arcprize/\narc-agi-benchmarking.\nARC-Prize. ARC-AGI leaderboard, 2025.https://arcprize.org/leaderboard.\nSusan Carey.The Origin of Concepts. MIT Press, Cambridge, MA, 2011.\nFrançois Chollet. On the measure of i...",
          "summary": "Page 11 is a reference list for a research paper, primarily citing works related to the Abstraction and Reasoning Corpus (ARC), AI benchmarking, cognitive science, and large language model evaluation. It includes publications from researchers like François Chollet, Douglas Hofstadter, and organizations such as ARC-Prize and OpenAI.",
          "entities": [
            "ARC-Prize",
            "François Chollet",
            "Douglas R. Hofstadter",
            "OpenAI",
            "MIT Press",
            "arXiv",
            "Nature Reviews Psychology",
            "Nature Machine Intelligence",
            "Proceedings of the International Conference on Machine Learning (ICML)",
            "Behavioral and brain sciences",
            "Transactions on Machine Learning Research",
            "Communications of the ACM",
            "Nature Human Behaviour",
            "Basic Books"
          ],
          "keywords": [
            "Abstraction and Reasoning Corpus (ARC)",
            "AI benchmarking",
            "large language models (LLMs)",
            "reasoning systems",
            "cognitive abilities",
            "shortcut learning",
            "multimodal reasoning",
            "human performance",
            "Bongard Problems"
          ],
          "key_points": [
            "The Abstraction and Reasoning Corpus (ARC) and its associated benchmarks (ARC-AGI, ARC-AGI-2) are central to the cited works.",
            "Several entries focus on evaluating the reasoning and cognitive capacities of AI systems, particularly large language models.",
            "The references highlight research into 'shortcut learning' in AI and methods for robust evaluation.",
            "Works by François Chollet are frequently cited, indicating his foundational role in the ARC benchmark.",
            "The list includes references to cognitive science literature, suggesting a connection between AI reasoning and human cognition."
          ],
          "technical_terms": [
            "ARC-AGI benchmarking",
            "ARC-AGI leaderboard",
            "Abstraction and Reasoning Corpus (ARC)",
            "ARC-AGI-Pub",
            "ARC-AGI-2",
            "shortcut learning",
            "large language models (LLMs)",
            "multimodal reasoning",
            "Bongard Problems",
            "ConceptARC benchmark",
            "H-ARC"
          ],
          "status": "success"
        },
        {
          "page": 12,
          "section": "Related Work",
          "char_count": 543,
          "text_preview": "Preprint. Under Review\nSunayana Rane, Cyrus Kirkman, Amanda Royka, Graham Todd, Ryan Law, Jacob Gates Foster, and\nErica Cartmill. Principles of animal cognition for LLM evaluations: A case study on transitive\ninference. InProceedings of the International Conference on Machine Learning, ICML-2025,\n20...",
          "summary": "This page lists two related works. The first, by Rane et al. (2025), proposes principles of animal cognition for LLM evaluations, using transitive inference as a case study. The second, by Zhang et al. (2019), introduces the RA VEN dataset for relational and analogical visual reasoning.",
          "entities": [
            "Sunayana Rane",
            "Cyrus Kirkman",
            "Amanda Royka",
            "Graham Todd",
            "Ryan Law",
            "Jacob Gates Foster",
            "Erica Cartmill",
            "Chi Zhang",
            "Feng Gao",
            "Baoxiong Jia",
            "Yixin Zhu",
            "Song-Chun Zhu",
            "ICML-2025",
            "RA VEN dataset",
            "IEEE/CVF Conference on Computer Vision and Pattern Recognition"
          ],
          "keywords": [
            "animal cognition",
            "LLM evaluations",
            "transitive inference",
            "relational reasoning",
            "analogical reasoning",
            "visual reasoning",
            "dataset"
          ],
          "key_points": [
            "Rane et al. (2025) propose using animal cognition principles for LLM evaluations, with a focus on transitive inference.",
            "Zhang et al. (2019) developed the RA VEN dataset for relational and analogical visual reasoning."
          ],
          "technical_terms": [
            "LLM evaluations",
            "transitive inference",
            "relational reasoning",
            "analogical reasoning",
            "visual reasoning"
          ],
          "status": "success"
        },
        {
          "page": 13,
          "section": "Related Work",
          "char_count": 1463,
          "text_preview": "Preprint. Under Review\nA TEXTUALPROMPT\nFind the common rule that maps an input grid to an output grid, given the examples below.\nExample 1\nInput:\n0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 4 4 4 4 0 0 0 0 0\n0 0 0 4 4 4 4 0 0 0 0 0\n0 0 0 4 4 4 4 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0\n0 0 0 0 0 0 0 0 0 0 0 0\n2 2 2 2 2 ...",
          "summary": "This page presents a textual prompt-based task where the goal is to identify a common rule that transforms an input grid into an output grid based on provided examples. It includes a \"No Tools Variant\" and a \"Tools Variant\" for solving the problem, and asks for a prediction on a test input grid in a specific JSON format.",
          "entities": [],
          "keywords": [
            "textual prompt",
            "input grid",
            "output grid",
            "common rule",
            "transformation rule",
            "test input grid",
            "JSON format"
          ],
          "key_points": [
            "The task is to find a common rule that maps an input grid to an output grid.",
            "Examples are provided to illustrate the input-output transformation.",
            "Two variants of the task are presented: 'No Tools Variant' and 'Tools Variant'.",
            "A test input grid is given for prediction.",
            "The output should be a minified JSON containing the identified rule and the predicted grid."
          ],
          "technical_terms": [
            "grid",
            "textual prompt",
            "JSON"
          ],
          "status": "success"
        },
        {
          "page": 14,
          "section": "Related Work",
          "char_count": 1200,
          "text_preview": "Preprint. Under Review\nB VISUALPROMPT\nThe left side of the first image shows 3 grids, where each grid square is colored with one of\n10 possible colors: black, blue, red, green, yellow, gray, magenta, orange, cyan or brown. The\nright side of the first image also contains 3 grids, each of which is a t...",
          "summary": "This page describes a visual reasoning task called VISUALPROMPT, which involves identifying a transformation rule between grids of colored squares and applying it to a new grid. It presents two variants: 'No Tools' and 'Tools', with the latter allowing Python usage. The output format is specified as a minified JSON object containing the rule and the final grid.",
          "entities": [
            "VISUALPROMPT"
          ],
          "keywords": [
            "visual reasoning",
            "transformation rule",
            "grid transformation",
            "colored squares",
            "pattern recognition",
            "applying rules",
            "natural language description",
            "JSON output"
          ],
          "key_points": [
            "The task is to determine a single rule describing transformations between grids of colored squares.",
            "The determined rule must be applied to a new test grid.",
            "Two variants exist: 'No Tools' (manual reasoning) and 'Tools' (allowing Python).",
            "The final output should be a minified JSON object with 'rule' and 'grid' keys."
          ],
          "technical_terms": [
            "grids",
            "colors",
            "transformation",
            "rule",
            "natural language",
            "indices",
            "JSON object",
            "minified"
          ],
          "status": "success"
        },
        {
          "page": 15,
          "section": "Related Work",
          "char_count": 1843,
          "text_preview": "Preprint. Under Review\nC PROMPTS FORNON-REASONING MODELS\nThe prompts we used for non-Reasoning models were minimally modified to require an additional\nfield containing a reasoning trace in the final JSON object. Otherwise, the prompts were consistent\nwith those used for reasoning models, including v...",
          "summary": "Page 15 details the prompt modifications for non-reasoning models, requiring an additional reasoning trace field in the JSON output. It also presents Table 2, which provides data for rule evaluation plots, showing the percentage of tasks classified by rule type (Correct-Intended, Correct-Unintended, Incorrect) for different models (o3, Claude Sonnet 4, Gemini 2.5 Pro) and human performance, partitioned by modality (Textual vs. Visual) and output grid correctness.",
          "entities": [
            "o3",
            "Claude Sonnet 4",
            "Gemini 2.5 Pro"
          ],
          "keywords": [
            "non-reasoning models",
            "reasoning trace",
            "JSON object",
            "rule evaluation",
            "rule classification",
            "modality",
            "output grid correctness",
            "textual",
            "visual"
          ],
          "key_points": [
            "Prompts for non-reasoning models were modified to include a reasoning trace in the JSON output.",
            "Table 2 presents data for rule evaluation plots, detailing task classifications by rule type, modality, and output grid correctness.",
            "Model percentages are computed over 480 total tasks, while human percentages are over approximately 4,175 total tests.",
            "Human responses with incorrect grids are listed as 'Not Classified' and are estimates based on reported grid accuracy.",
            "The final row for humans shows rule classification excluding 'Not Classified' rules."
          ],
          "technical_terms": [
            "non-Reasoning models",
            "reasoning trace",
            "JSON object",
            "rule classification",
            "Correct-Intended",
            "Correct-Unintended",
            "Incorrect",
            "modality",
            "Textual",
            "Visual",
            "Correct Grid",
            "Incorrect Grid",
            "Not Classified"
          ],
          "status": "success"
        }
      ],
      "total_pages": 15,
      "total_chars": 45427,
      "total_entities": 126,
      "total_keywords": 130,
      "llm_successes": 15,
      "llm_failures": 0,
      "aggregate_summary": "This paper investigates the abstraction abilities of AI models, particularly OpenAI's o3-preview, using the ConceptARC benchmark. It evaluates models across different input modalities (textual vs. visual) and tool usage, assessing both output accuracy and the natural-language rules generated to explain solutions. The findings suggest that while text-based models can match human accuracy, their reasoning often relies on surface-level patterns, overestimating their abstract reasoning capabilities...."
    }
  }
}